{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc2fcf66-a275-479e-8d6e-20baac59d8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from collections import deque\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31283171-0274-4a4e-9a04-527cc6079400",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient\n",
    "\n",
    "DDPG is an Actor-Critic RL algorithm. It has a policy approximation model and a value approximation model. Its policy network produces continuous actions, and combines them with a DQN-like q-value estimator.\n",
    "\n",
    "DDPG uses 2 main models. The Actor network learns the policy and the Critic network learns the q-value. The Actor network receives the state as input and outputs an action vector, corresponding to the continuous action space (e.g. joint velocities), thus it produces a deterministic policy. The Critic receives the output of the Actor, combines it with the state, and approximates a q-value. \n",
    "\n",
    "Since the learned the policy is deterministic, we need to implement some kind of exploration during the training process. To make DDPG policies explore better, a noise to their actions. Originally an Ornstein-Uhlenbeck (https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.) noise was proposed, however it is often used with normal-distributed noise or parameter noise, directly applied to the network parameters.\n",
    "\n",
    "## Agent\n",
    "\n",
    "Similarly to the PolicyGradient agent, the DDPG agent also has \"frozen\" models. Both the Actor and the Critic network have their target Actor and target Critic counterparts.\n",
    "\n",
    "In case of the Critic, the learning step is almost identical to the learning step of the DQN. The only difference is, that here, the actions are directly fed into the network, as input, and the output is the corresponding q-value, instead of having multiple q-values for each possible action.\n",
    "\n",
    "The interesting part is the training of the Actor. Since the goal is to maximize the q-value with the policy, and we already have a differentiable q-value approximator, the solution is sort of obvious. We just feed the output of the Actor $\\pi_\\phi$ into our Critic $c_\\theta$, along with the state, negate the result and use it as loss.\n",
    "\n",
    "$$\n",
    "loss_{actor} = -c_\\Theta (s_t, \\pi_\\Phi (s_t))\n",
    "$$\n",
    "\n",
    "If the Critic is doing its job well, and the Actor minimizes the negated output of the Critic, then it should maximize a well approximated q-value, thus also approaching a well working policy.\n",
    "\n",
    "## Replay buffer\n",
    "\n",
    "Similar to the case of the DQN, a replay buffer is used to store gathered experience, and later sample from it during the training phase. In this case however, we adjust the samplint method to faver resent experiences over older experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cda979ca-9d0f-495f-8870-29cee1b60464",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=(400, 300), n_actions=2, **kwargs):\n",
    "        super(Actor, self).__init__(**kwargs)\n",
    "        self.layers = []\n",
    "        for i, u in enumerate(units):\n",
    "            self.layers.append(tf.keras.layers.Dense(u, activation=tf.nn.leaky_relu,\n",
    "                                                     kernel_initializer=tf.keras.initializers.glorot_normal()))\n",
    "        last_init = tf.random_normal_initializer(stddev=0.0005)\n",
    "        self.layers.append(tf.keras.layers.Dense(n_actions, activation='tanh', kernel_initializer=last_init))\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = inputs\n",
    "        for l in self.layers:\n",
    "            outputs = l(outputs)\n",
    "        return outputs\n",
    "    \n",
    "class Critic(tf.keras.layers.Layer):\n",
    "    def __init__(self, state_units=(400, 300), action_units=(300,), units=(150,), **kwargs):\n",
    "        super(Critic, self).__init__(**kwargs)\n",
    "        self.layers_state = []\n",
    "        for u in state_units:\n",
    "            self.layers_state.append(tf.keras.layers.Dense(u, activation=tf.nn.leaky_relu,\n",
    "                                                           kernel_initializer=tf.keras.initializers.glorot_normal()))\n",
    "\n",
    "        self.layers_action = []\n",
    "        for u in action_units:\n",
    "            self.layers_action.append(tf.keras.layers.Dense(u, activation=tf.nn.leaky_relu,\n",
    "                                                            kernel_initializer=tf.keras.initializers.glorot_normal()))\n",
    "\n",
    "        self.layers = []\n",
    "        for u in units:\n",
    "            self.layers.append(tf.keras.layers.Dense(u, activation=tf.nn.leaky_relu,\n",
    "                                                     kernel_initializer=tf.keras.initializers.glorot_normal()))\n",
    "        last_init = tf.random_normal_initializer(stddev=0.00005)\n",
    "        self.layers.append(tf.keras.layers.Dense(1, kernel_initializer=last_init))\n",
    "\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        p_action = inputs['action']\n",
    "        p_state = inputs['state']\n",
    "\n",
    "        for l in self.layers_action:\n",
    "            p_action = l(p_action)\n",
    "\n",
    "        for l in self.layers_state:\n",
    "            p_state = l(p_state)\n",
    "\n",
    "        outputs = self.add([p_state, p_action])\n",
    "        for l in self.layers:\n",
    "            outputs = l(outputs)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "                self.x_prev\n",
    "                + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "                + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)\n",
    "            \n",
    "class DDPGAgent:\n",
    "    def __init__(self, action_space, observation_shape, gamma=0.99, tau=0.001, epsilon=0.05):\n",
    "        self.action_space = action_space\n",
    "        self.tau = tau  # target network weight adaptation\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.actor = Actor(n_actions=action_space.shape[0])\n",
    "        self.critic = Critic()\n",
    "\n",
    "        self.target_actor = Actor(n_actions=action_space.shape[0])\n",
    "        self.target_critic = Critic()\n",
    "\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "        self.noise = OUActionNoise(mean=np.zeros(np.array(self.action_space.sample()).shape),\n",
    "                                   std_deviation=float(0.2) * np.ones(1))\n",
    "\n",
    "        self._init_networks(observation_shape)\n",
    "\n",
    "    def _init_networks(self, observation_shape):\n",
    "        initial_state = np.zeros([1, observation_shape])\n",
    "\n",
    "        initial_action = self.actor(initial_state)\n",
    "        self.target_actor(initial_state)\n",
    "\n",
    "        critic_input = {'action': initial_action, 'state': initial_state}\n",
    "        self.critic(critic_input)\n",
    "        self.target_critic(critic_input)\n",
    "\n",
    "        self.target_actor.set_weights(self.actor.get_weights())\n",
    "        self.target_critic.set_weights(self.critic.get_weights())\n",
    "\n",
    "    @staticmethod\n",
    "    def update_target(model_target, model_ref, tau=0.0):\n",
    "        new_weights = [tau * ref_weight + (1 - tau) * target_weight for (target_weight, ref_weight) in\n",
    "                       list(zip(model_target.get_weights(), model_ref.get_weights()))]\n",
    "        model_target.set_weights(new_weights)\n",
    "\n",
    "    def act(self, observation, explore=True, random_action=False):\n",
    "        if random_action or np.random.uniform(0, 1) < self.epsilon:\n",
    "            a = self.action_space.sample()\n",
    "        else:\n",
    "            a = self.actor(observation).numpy()[:, 0]\n",
    "            if explore:\n",
    "                a += self.noise()\n",
    "        a = np.clip(a, self.action_space.low, self.action_space.high)\n",
    "        return a\n",
    "\n",
    "    def compute_target_q(self, rewards, next_states, dones):\n",
    "        actions = self.target_actor(next_states)\n",
    "        critic_input = {'action': actions, 'state': next_states}\n",
    "        next_q = self.target_critic(critic_input)\n",
    "        target_q = rewards + (1 - dones) * next_q * self.gamma\n",
    "        return target_q\n",
    "\n",
    "    def get_actor_grads(self, states):\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.actor(states)\n",
    "            critic_input = {'action': actions, 'state': states}\n",
    "            qs = self.critic(critic_input)\n",
    "            loss = -tf.math.reduce_mean(qs)\n",
    "        gradients = tape.gradient(loss, self.actor.trainable_variables)\n",
    "        gradients = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gradients]\n",
    "        return gradients, loss\n",
    "\n",
    "    def get_critic_grads(self, states, actions, target_qs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            critic_input = {'action': actions, 'state': states}\n",
    "            qs = self.critic(critic_input)\n",
    "            loss = tf.reduce_mean(tf.abs(target_qs - qs))\n",
    "        gradients = tape.gradient(loss, self.critic.trainable_variables)\n",
    "        gradients = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gradients]\n",
    "        return gradients, loss\n",
    "\n",
    "    def learn(self, states, actions, rewards, next_states, dones):\n",
    "        target_qs = self.compute_target_q(rewards, next_states, dones)\n",
    "\n",
    "        actor_grads, actor_loss = self.get_actor_grads(states)\n",
    "        critic_grads, critic_loss = self.get_critic_grads(states, actions, target_qs)\n",
    "\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n",
    "        self.target_update()\n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "    def target_update(self):\n",
    "        DDPGAgent.update_target(self.target_critic, self.critic, self.tau)\n",
    "        DDPGAgent.update_target(self.target_actor, self.actor, self.tau)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.p_indices = [0.5 / 2]\n",
    "\n",
    "    def put(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append([state, action, np.expand_dims(reward, -1), next_state, np.expand_dims(done, -1)])\n",
    "\n",
    "    def sample(self, batch_size=1, unbalance=0.8):\n",
    "        p_indices = None\n",
    "        if random.random() < unbalance:\n",
    "            self.p_indices.extend((np.arange(len(self.buffer) - len(self.p_indices)) + 1)\n",
    "                                  * 0.5 + self.p_indices[-1])\n",
    "            p_indices = self.p_indices / np.sum(self.p_indices)\n",
    "        sample_idx = np.random.choice(len(self.buffer),\n",
    "                                      size=min(batch_size, len(self.buffer)),\n",
    "                                      replace=False,\n",
    "                                      p=p_indices)\n",
    "        sample = [self.buffer[s_i] for s_i in sample_idx]\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*sample))\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2060c7-ad34-49be-b975-98bebb7cc60b",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The training loop is similar to the other training loops we saw until now. Except, for periodically updating and adapting the parameter noise, and we let the agent collect some initial experience in the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8b78f6a-5345-4ab1-aaac-8b2cf363a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(env, agent, num_episodes=1, max_steps=200, render=False):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        episode_return = 0.0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        while not (done or steps > max_steps):\n",
    "            if render:\n",
    "                clear_output(wait=True)\n",
    "                plt.axis('off')\n",
    "                plt.imshow(env.render())\n",
    "                plt.show()\n",
    "            action = agent.act(np.array([obs]))\n",
    "            obs, r, done, _, _ = env.step(action)\n",
    "            episode_return += r\n",
    "            steps += 1\n",
    "        total_return += episode_return\n",
    "    return total_return / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0952a592-1234-4e11-aba7-6c3545190bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\gerge\\Anaconda3\\envs\\tacs39\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, actor loss 2.2243731021881104, critic loss 0.584580659866333 , avg return -1065.7131428494852\n",
      "epoch 25, actor loss 14.828900337219238, critic loss 0.23707401752471924 , avg return -1720.714891303015\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "env = gym.make('Pendulum-v1', render_mode='rgb_array')\n",
    "\n",
    "agent = DDPGAgent(env.action_space, env.observation_space.shape[0])\n",
    "for i in range(1001):\n",
    "    obs, _ = env.reset()\n",
    "    # gather experience\n",
    "    agent.noise.reset()\n",
    "    ep_actor_loss = 0\n",
    "    ep_critic_loss = 0\n",
    "    steps = 0\n",
    "    for j in range(200):\n",
    "        steps += 1\n",
    "        env.render()\n",
    "        action = agent.act(np.array([obs]), random_action=(i < 1))\n",
    "        # execute action\n",
    "        new_obs, r, done, _, _ = env.step(action)\n",
    "        replay_buffer.put(obs, action, r, new_obs, done)\n",
    "        obs = new_obs\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    # Learn from the experiences in the replay buffer.\n",
    "    for _ in range(128):\n",
    "        s_states, s_actions, s_rewards, s_next_states, s_dones = replay_buffer.sample(64)\n",
    "        actor_l, critic_l = agent.learn(s_states, s_actions, s_rewards, s_next_states, s_dones)\n",
    "        ep_actor_loss += actor_l\n",
    "        ep_critic_loss += critic_l\n",
    "        \n",
    "    if i % 25 == 0:\n",
    "        avg_return = compute_avg_return(env, agent, num_episodes=2, render=False)\n",
    "        print(\n",
    "            f'epoch {i}, actor loss {ep_actor_loss / steps}, critic loss {ep_critic_loss / steps} , avg return {avg_return}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c387af69-45de-4ff8-a7db-43b01eda44dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_avg_return(env, agent, num_episodes=10, render=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0916dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
