{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e7155ef-c540-4914-9a77-01fbaf08fd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.stats import zscore\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d867865-0e8a-4df2-b28c-93650e047fc4",
   "metadata": {},
   "source": [
    "# Policy Gradient RL\n",
    "\n",
    "DQN is a value based method. It approximates the action value function and from that, it derives a policy.\n",
    "\n",
    "What if we wanted to learn the policy itself?\n",
    "\n",
    "Why would we want to do that? Why is DQN not enough?\n",
    "\n",
    "First of all, DQNs approximate the action value function. Sometimes, a value function can be very complex making it unfeasable to learn. Secondly, DQNs produce deterministic policies, with policy gradient methods we can generate stochastic policies\n",
    "\n",
    "## Agent\n",
    "\n",
    "We want to maximize the expected discounted cumulative reward. Let us define the function \n",
    "\n",
    "$$\n",
    "J(\\Theta)=E[\\sum_t y^tR_t]\n",
    "$$\n",
    "\n",
    "that maps the parameters $\\Theta$ of our neural network to this expected discounted cumulative reward. Now, if we would want to update our networks wieghts to maximize this value, we could simply do a gradient ascent:\n",
    "\n",
    "$$\n",
    "\\Theta \\leftarrow \\Theta + \\nabla_\\Theta J(\\Theta)\n",
    "$$\n",
    "\n",
    "As it turns out, after some derivation (well explained here: https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63) the gradient that we would need is equal to\n",
    "\n",
    "$$\n",
    "\\nabla_\\Theta J(\\Theta) = \\sum_t \\nabla_\\Theta \\log \\pi_\\Theta (a_t | s_t) G_t\n",
    "$$\n",
    "\n",
    "with $G_t$ the discounted cumulative reward as of time step $t+1$import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.stats import zscore\n",
    "import random\n",
    "\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True).\n",
    "\n",
    "Luckily, the expression (let us call it the log policy)\n",
    "\n",
    "$$\n",
    "\\log \\pi_\\Theta (a_t | s_t)\n",
    "$$\n",
    "\n",
    "is exactly the negated cross entropy of the probability distribution of the action taken at time step t (basically a one-hot encoding of $a_t$) and the probability distribution over the actions from our current policy given the state $s_t$. Since our policy is a neural network, we can differenciate it, thus we can also calculate the gradients we need.\n",
    "\n",
    "So basically, if we complete an episode, and record all the states we have visited, all the actions we have taken and all the rewards we have received, we can calculate the discounted cumulative reward and we can plug everything into the equation above. Additionally, since we would use cross entropy to calculate the log policy, it is already negated, thus we can simply use gradient descent as usual.\n",
    "\n",
    "So let's get to the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c430add-d4f4-485b-8a1f-659378f465ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientN(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=(12, 12), n_actions=2, **kwargs):\n",
    "        super(PolicyGradientN, self).__init__(**kwargs)\n",
    "        self.layers = []\n",
    "        for u in units:\n",
    "            self.layers.append(tf.keras.layers.Dense(u, activation='relu'))\n",
    "        self.layers.append(tf.keras.layers.Dense(n_actions))\n",
    "\n",
    "            \n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = inputs\n",
    "        for l in self.layers:\n",
    "            outputs = l(outputs)\n",
    "        return outputs\n",
    "    \n",
    "class PolicyGradientAgent:\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "        self.model = PolicyGradientN(n_actions=self.action_space.n)\n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "        \n",
    "    def act(self, observation):\n",
    "        probs = tf.nn.softmax(self.model(observation)).numpy()\n",
    "        probs = np.squeeze(probs)\n",
    "        return probs\n",
    "        \n",
    "    def learn(self, states, actions, discounted_cumulated_rewards):\n",
    "        with tf.GradientTape() as tape:\n",
    "            probs = self.model(states)\n",
    "            log_policy = tf.nn.sparse_softmax_cross_entropy_with_logits(actions, probs)\n",
    "            loss = tf.reduce_mean(discounted_cumulated_rewards * log_policy)\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        gradients = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gradients]\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        return loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d40717-1959-4588-8949-adfe494823f5",
   "metadata": {},
   "source": [
    "You might have noticed, that we have no $\\epsilon$ greedy policy, we have no explore flag in the act method. The reason for that is, that we already have a stochastic policy. We sample from a distribution, thus there is allways some probability to chosing different actions given a state. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6f4839-64e4-4af4-ae5e-d36c4808e349",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We have no replay buffer for training policy gradient methods. We allways complete a full episode, record the trajectory and the rewards. We then use these to update our network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6960ed06-9e88-40a8-b8eb-bd2860f0bff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(env, agent, num_episodes=1, max_steps=200, render=False):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        episode_return = 0.0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        while not (done or steps > max_steps):\n",
    "            if render:\n",
    "                clear_output(wait=True)\n",
    "                plt.axis('off')\n",
    "                plt.imshow(env.render())\n",
    "                plt.show()\n",
    "            probs = agent.act(np.array([obs]))\n",
    "            action = np.random.choice(env.action_space.n, p=probs)\n",
    "            obs, r, done, _, _ = env.step(action)\n",
    "            episode_return += r\n",
    "            steps += 1\n",
    "        total_return += episode_return\n",
    "    return total_return / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30a36614-0456-411d-8392-ee2c7173e4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\gerge\\Anaconda3\\envs\\tacs39\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, loss 0.042639490216970444, avg_return 23.4\n",
      "episode 200, loss 0.027808163315057755, avg_return 49.7\n",
      "episode 400, loss 0.009050794877111912, avg_return 101.9\n",
      "episode 600, loss -0.032088395208120346, avg_return 186.2\n",
      "episode 800, loss -0.00261726719327271, avg_return 192.2\n",
      "episode 1000, loss -0.019606685265898705, avg_return 198.5\n",
      "episode 1200, loss -0.0043202778324484825, avg_return 201.0\n",
      "episode 1400, loss -0.016649501398205757, avg_return 201.0\n",
      "episode 1600, loss -0.025335771963000298, avg_return 201.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "\n",
    "agent = PolicyGradientAgent(env.action_space)\n",
    "\n",
    "for i in range(1601):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    \n",
    "    # steps = 0\n",
    "    while not done:\n",
    "        # steps += 1\n",
    "        probs = agent.act(np.array([obs]))\n",
    "        action = np.random.choice(env.action_space.n, p=probs)\n",
    "        # execute action\n",
    "        new_obs, r, done, _, _ = env.step(action)\n",
    "        # if done and steps < env._max_episode_steps: \n",
    "            # r = -100\n",
    "        states.append(obs)\n",
    "        rewards.append(r)\n",
    "        actions.append(action)\n",
    "        obs = new_obs\n",
    "    \n",
    "    # compute the discounted cumulated rewards for the whole epoch\n",
    "    discounted_cumulated_rewards = np.zeros_like(rewards)\n",
    "    s = 0.0\n",
    "    for j in reversed(range(len(rewards))):\n",
    "        s = s * 0.99 + rewards[j]\n",
    "        discounted_cumulated_rewards[j] = s\n",
    "    # to make the training more stable, we compute the zscore of the discounted cumulated rewards \n",
    "    discounted_cumulated_rewards = zscore(discounted_cumulated_rewards)\n",
    "\n",
    "    # shuffle the training data\n",
    "    temp = list(zip(states, discounted_cumulated_rewards, actions))\n",
    "    random.shuffle(temp)\n",
    "    states, discounted_acc_rewards, actions = zip(*temp)\n",
    "    loss = agent.learn(np.array(states), actions, discounted_acc_rewards)\n",
    "    \n",
    "    if i % 200 == 0:\n",
    "        avg_return = compute_avg_return(env, agent, num_episodes=10)\n",
    "        print(f'episode {i}, loss {loss}, avg_return {avg_return}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47de6c63-7a00-4ef0-9560-c39653f6e0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compute_avg_return(env, agent, num_episodes=2, render=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1d024b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
