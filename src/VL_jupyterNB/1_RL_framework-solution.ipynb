{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e609411-8095-4610-9412-950a16cf735b",
   "metadata": {},
   "source": [
    "# RL Framework\n",
    "\n",
    "- Environment\n",
    "- Agent\n",
    "\n",
    "<div>\n",
    "<img src=\"img/rl_base.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b038dc-078c-4510-8d9e-b8ef6576b245",
   "metadata": {},
   "source": [
    "## Environment\n",
    "### GridWorld\n",
    "GridWorld is a 2D rectangular grid of size NxM. It has an **agent** starting in one of the grid squares and possible **rewards** in other grid squares.\n",
    "\n",
    "In our initial setup, the GridWorld is a 3x4 grid with the agent starting in the bottom left corner. The world contains a blocking state, a positive and a negative reward.\n",
    "\n",
    "The agent's **goal** is to receive a positive reward by moving up, down, left or right. The game ends when a reward is received.\n",
    "\n",
    "<div>\n",
    "<img src=\"img/grid_example.png\" width=\"250\"/>\n",
    "</div>\n",
    "\n",
    "Complete the **TODO**s in the code, marked with `...`, and discuss some questions, marked with **TODO** in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085bbfd8",
   "metadata": {},
   "source": [
    "### Step 1 (TODO): Define GridWorld Parameters\n",
    "Define variables for the dimensions of the GridWorld, the initial position of the agent, the list of blocking state positions, and the dictionary of rewards with key: position and value: reward.\n",
    "\n",
    "Use the following values:\n",
    "\n",
    "- dimensions of the GridWorld: (3, 4)\n",
    "- initial position of the agent: (2, 0)\n",
    "- list of blocking state positions: [(1, 1)]\n",
    "- dictionary of rewards: {(0,3): 1, (1,3): -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6920753b-8605-4339-a226-285adbcf67b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions of the GridWorld.\n",
    "world_shape = (3, 4)\n",
    "# Initial position of the agent.\n",
    "agent_init_pos = (2, 0)\n",
    "\n",
    "# TODO: Define the remaining variables.\n",
    "# List of blocking state positions.\n",
    "blocking_states = [(1, 1)]\n",
    "# Dictionary of rewards with key: position and value: reward.\n",
    "reward_states = {(0,3): 1, (1,3): -1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf423e85-6d61-492b-b907-0651acbfac34",
   "metadata": {},
   "source": [
    "### Step 2 (TODO): Visualization\n",
    "For now, only a human agent will interact with our environment.\n",
    "\n",
    "We need some visualizations.\n",
    "\n",
    "The environment is represented by a 2D array. We will label\n",
    "- empty states with 0,\n",
    "- the agent with 4,\n",
    "- blocking states 8,\n",
    "- rewards with their respective values.\n",
    "\n",
    "Your task is to define the function `render_environment`, that takes in the world shape, agent position, blocking states, and reward states, and returns a 2D array representing the environment. Follow the instructions in the code comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a433090b-65ed-4c0b-b8d9-f42cd465ac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "legend = {\n",
    "    'empty': 0,\n",
    "    'agent': 4,\n",
    "    'blocking': 8\n",
    "}\n",
    "\n",
    "def render_environment(world_shape, agent_pos, blocking_states, reward_states):\n",
    "    # Initialize empty states.\n",
    "    states = np.ones(world_shape) * legend['empty']\n",
    "    \n",
    "    # Add agent.\n",
    "    # We can index states with agent_pos because is a tuple of ints.\n",
    "    # You can not index a dictionary with numpy arrays.\n",
    "    # Make sure, that everytime you call this function, the indexing values are tuples of ints.\n",
    "    states[agent_pos] = legend['agent']\n",
    "    \n",
    "    # TODO: Add blocking states.\n",
    "    # Iterate over blocking_states, and set the states value according to the legend.\n",
    "    # blocking_states is a list of tuples of ints.\n",
    "    for blocking_state in blocking_states:\n",
    "        states[blocking_state] = legend['blocking']\n",
    "    \n",
    "    # TODO: Add rewards.\n",
    "    # Iterate over reward_states dictionary's items.\n",
    "    for reward_state, reward in reward_states.items():\n",
    "        states[reward_state] = reward\n",
    "\n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1ea39c",
   "metadata": {},
   "source": [
    "Now, call the `render_environment` function with the GridWorld parameters defined earlier, and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1292f32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  1.]\n",
      " [ 0.  8.  0. -1.]\n",
      " [ 4.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "render = render_environment(world_shape, agent_init_pos, blocking_states, reward_states)\n",
    "print(render)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c8841d-a043-464f-8905-bf71e8045fd7",
   "metadata": {},
   "source": [
    "### Step 3 (TODO): Actions\n",
    "Possible actions in the GridWorld:\n",
    "- up\n",
    "- down\n",
    "- right\n",
    "- left\n",
    "\n",
    "the agent is blocked by the bounds of the GridWorld and blocking states.\n",
    "\n",
    "Define a dictionary possible_actions with keys: 'up', 'down', 'right', 'left', and values: numpy arrays that represent the corresponding movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99fe00de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the possible_actions dictionary.\n",
    "possible_actions = {\n",
    "    'up': np.array([-1, 0]),\n",
    "    'right': np.array([0, 1]),\n",
    "    'down': np.array([1, 0]),\n",
    "    'left': np.array([0, -1])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d5559",
   "metadata": {},
   "source": [
    "Now, define the function `move_agent` that takes in the agent's position, the action to perform, the shape of the world, and the blocking states, and returns the new position of the agent. Follow the instructions in the code comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d805d87f-96e3-456f-9f2d-b89c63f700a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_agent(agent_pos, action, world_shape, blocking_states):\n",
    "    # Move agent.\n",
    "    # We cast agent_pos to numpy array, so that we can do maths properly.\n",
    "    new_agent_pos = np.array(agent_pos) + possible_actions[action]\n",
    "    \n",
    "    # TODO: Check if new position is blocked, by checking whether the new agent position\n",
    "    # is in a blocking state. If it is, return the unchanged agent_pos. \n",
    "    if tuple(new_agent_pos) in blocking_states:\n",
    "        return agent_pos\n",
    "    \n",
    "    # TODO: Check if new position is out of bounds. If it is, return the unchanged agent_pos.\n",
    "    # Possible coordinates are in the [0, world_shape[0] - 1] and [0, world_shape[1] - 1] intervals.\n",
    "    if (new_agent_pos < (0, 0)).any() or (new_agent_pos >= world_shape).any():\n",
    "        return agent_pos\n",
    "    \n",
    "    # We cast new_agent_pos to have no problems during rendering.\n",
    "    return tuple(new_agent_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783836dc",
   "metadata": {},
   "source": [
    "Test the `move_agent` function with some actions and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfb2dbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  1.]\n",
      " [ 0.  8.  0. -1.]\n",
      " [ 4.  0.  0.  0.]]\n",
      "going down\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 0.  8.  0. -1.]\n",
      " [ 4.  0.  0.  0.]]\n",
      "going up\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 4.  8.  0. -1.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "going right\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 4.  8.  0. -1.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "going left\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 4.  8.  0. -1.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "going up\n",
      "[[ 4.  0.  0.  1.]\n",
      " [ 0.  8.  0. -1.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "going up\n",
      "[[ 4.  0.  0.  1.]\n",
      " [ 0.  8.  0. -1.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Test some actions\n",
    "actions = ['down', 'up', 'right', 'left', 'up', 'up']\n",
    "new_agent_pos = agent_init_pos\n",
    "render = render_environment(world_shape, new_agent_pos, blocking_states, reward_states)\n",
    "print(render)\n",
    "for action in actions:\n",
    "    print(f'going {action}')\n",
    "    new_agent_pos = move_agent(new_agent_pos, action, world_shape, blocking_states)\n",
    "    render = render_environment(world_shape, new_agent_pos, blocking_states, reward_states)\n",
    "    print(render)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cbf5f2-2fa0-47a0-82a2-eff7ea315e88",
   "metadata": {},
   "source": [
    "### Step 4 (TODO): Implement the GridWorld Class\n",
    "We have everything for the environment.\n",
    "\n",
    "Now, your task is to implement the GridWorld class, which will encapsulate all the information about the environment (dimensions, agent_position ...) and provides the methods \n",
    "- <code>reset</code> the GridWorld to its initial state\n",
    "- <code>step</code> the environment by executing actions, returning new observations, calculating rewards and deciding whether the game has ended\n",
    "\n",
    "to interact with it. Follow the instructions in the code comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7e28377-a8f9-414f-880a-f31e810f4711",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, world_shape, agent_init_pos, blocking_states, reward_states):\n",
    "        # TODO: Initialize the class attributes.\n",
    "        self.world_shape = world_shape\n",
    "        self.agent_init_pos = agent_init_pos\n",
    "        self.blocking_states = blocking_states\n",
    "        self.reward_states = reward_states\n",
    "        \n",
    "        # TODO: Initialize the agent's current position to its initial position.\n",
    "        self.agent_current_pos = self.agent_init_pos\n",
    "        \n",
    "    def reset(self):\n",
    "        # TODO: Reset agent position.\n",
    "        self.agent_current_pos = self.agent_init_pos\n",
    "            \n",
    "        # TODO: Render initial observation.\n",
    "        # Use the method render_environment from before.\n",
    "        observation = render_environment(self.world_shape, self.agent_current_pos, self.blocking_states, self.reward_states)\n",
    "        \n",
    "        return observation\n",
    "        \n",
    "    def step(self, action):\n",
    "        # TODO: Execute action and update agent_current_pos.\n",
    "        # Use the method move_agent from before.\n",
    "        self.agent_current_pos = move_agent(self.agent_current_pos, action, self.world_shape, self.blocking_states)\n",
    "        \n",
    "        # Check if there is any reward and whether the game ended. If the game has \n",
    "        # ended, set done flag to True.\n",
    "        if self.agent_current_pos in self.reward_states.keys():\n",
    "            done = True\n",
    "            reward = self.reward_states[self.agent_current_pos]\n",
    "        else:\n",
    "            done = False\n",
    "            reward = 0\n",
    "            \n",
    "        # TODO: Render observation\n",
    "        # Use the method render_environment again.\n",
    "        observation = render_environment(self.world_shape, self.agent_current_pos, self.blocking_states, self.reward_states)\n",
    "        return observation, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d95e24c-46bb-4c66-97e9-9c6680bf040c",
   "metadata": {},
   "source": [
    "## Agent\n",
    "We use python's <code>input()</code> method to receive actions from the human agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0643b07",
   "metadata": {},
   "source": [
    "### Step 5: Define Function to Receive Input\n",
    "We define the function `receive_action_input` that asks the user for input and returns the corresponding action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb6fc7bc-020f-47bb-abca-502f60df0ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def receive_action_input():\n",
    "    # Read input and translate to action.\n",
    "    action = input('move with w, a, s, d; exit with q - ')\n",
    "    if action == 'w':\n",
    "        return 'up'\n",
    "    elif action == 'a':\n",
    "        return 'left'\n",
    "    elif action == 's':\n",
    "        return 'down'\n",
    "    elif action == 'd':\n",
    "        return 'right'\n",
    "    # Additional action to exit from GridWorld\n",
    "    elif action == 'q':\n",
    "        return 'exit'\n",
    "    # Handle other cases.\n",
    "    else:\n",
    "        print(f'Invalid input - {action}')\n",
    "        return receive_action_input()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0362cb",
   "metadata": {},
   "source": [
    "Test the `receive_action_input` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f47019ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "move with w, a, s, d; exit with q - q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'exit'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "receive_action_input()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daad8708-eaf2-4f43-88f6-58172fae0350",
   "metadata": {},
   "source": [
    "Now, we define the function `act` that presents the observation to the human agent and obtains an action from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef35231b-f3ca-4a4b-b56e-dc1fd723f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(observation):\n",
    "    # Present observation to human agent.\n",
    "    print(observation)\n",
    "    \n",
    "    # Obtain action from human agent.\n",
    "    action = receive_action_input()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68df1d87-ecd0-4c88-9356-dab87b0c6055",
   "metadata": {},
   "source": [
    "## Agent Environment Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a70c306",
   "metadata": {},
   "source": [
    "### Step 6 (TODO): Implement Agent-Environment Interaction\n",
    "Your task is to implement the agent-environment interaction loop. In each iteration of the loop, get an action from the agent, execute it in the environment, and handle the consequences. Follow the instructions in the code comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfa48de3-e098-4ffb-bb8f-78b581974e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  1.]\n",
      " [ 0.  8.  0. -1.]\n",
      " [ 4.  0.  0.  0.]]\n",
      "move with w, a, s, d; exit with q - w\n",
      "went up, received reward 0\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 4.  8.  0. -1.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "move with w, a, s, d; exit with q - w\n",
      "went up, received reward 0\n",
      "[[ 4.  0.  0.  1.]\n",
      " [ 0.  8.  0. -1.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "move with w, a, s, d; exit with q - d\n",
      "went right, received reward 0\n",
      "[[ 0.  4.  0.  1.]\n",
      " [ 0.  8.  0. -1.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "move with w, a, s, d; exit with q - d\n",
      "went right, received reward 0\n",
      "[[ 0.  0.  4.  1.]\n",
      " [ 0.  8.  0. -1.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "move with w, a, s, d; exit with q - d\n",
      "went right, received reward 1\n",
      "============= game over =============\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 0.  8.  0. -1.]\n",
      " [ 4.  0.  0.  0.]]\n",
      "move with w, a, s, d; exit with q - q\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment.\n",
    "env = GridWorld(world_shape, agent_init_pos, blocking_states, reward_states)\n",
    "\n",
    "# Reset environment and receive initial observaion.\n",
    "obs = env.reset()\n",
    "# We loop until the agent exits the game\n",
    "while True:\n",
    "    # TODO: Get action from agent.\n",
    "    # use the act method.\n",
    "    action = act(obs)\n",
    "    \n",
    "    # Exit loop if agent's action is exit.\n",
    "    if action == 'exit':\n",
    "        break\n",
    "    \n",
    "    # TODO: Execute action in environment.\n",
    "    # Use the env objects step method. It takes action as a parameter and returns\n",
    "    # the observation, reward and a done flag.\n",
    "    obs, reward, done = env.step(action)\n",
    "    \n",
    "    # Print action and reward.\n",
    "    print(f'went {action}, received reward {reward}')\n",
    "    \n",
    "    # Reset environment if game ended\n",
    "    if done:\n",
    "        print('============= game over =============')\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0e4863-2ac6-4d97-8fd8-b2a23b39200d",
   "metadata": {},
   "source": [
    "### Step 7 (TODO): Value\n",
    "Define a function calculate_values that calculates the cumulative future reward for each step from a list of rewards. Follow the instructions in the code comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d57693d2-b3e9-41fd-b941-19693220f265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_values(rewards):\n",
    "    # Empty list to store the values.\n",
    "    values = []\n",
    "    # Iterate over the indices of rewards, but skip the last one because there is no future reward\n",
    "    for i in range(len(rewards) - 1):\n",
    "        # TODO: Calculate the sum of future rewards.\n",
    "        value = np.sum(rewards[i + 1:])\n",
    "        # TODO: Append the calculated value to values.\n",
    "        values.append(value)\n",
    "    # TODO: Append 0 as future reward for the last step.\n",
    "    values.append(0)\n",
    "    return values\n",
    "\n",
    "# TODO (Optional): calculate values using numpy's cumulative sum (cumsum) function.\n",
    "def calculate_values_cumsum(rewards):\n",
    "    # TODO: Shift reward indices and set last reward to be 0, there is no following step, thus also no reward.\n",
    "    shifted_rewards = rewards[1:] + [0]\n",
    "    # TODO: convert list of shifted rewards to numpy array\n",
    "    rewards_np = np.array(shifted_rewards)\n",
    "    # TODO: Reverse rewards.\n",
    "    rewards_np = rewards_np[::-1]\n",
    "    # TODO: Compute cumulative sum.\n",
    "    values = np.cumsum(rewards_np)\n",
    "    # TODO: Reverse values.\n",
    "    values = values[::-1]\n",
    "    return values\n",
    "\n",
    "# TODO (Optional): calculate values using the recursive definition.\n",
    "def calculate_values_recursive(rewards):\n",
    "    if len(rewards) == 1:\n",
    "        # TODO: If last step, there is no future reward, return a list containing only a 0\n",
    "        return [0]\n",
    "    else:\n",
    "        # TODO: Compute values for the next step.\n",
    "        next_values = calculate_values_recursive(rewards[1:])\n",
    "        # TODO: Use the recursive definition to compute current value.\n",
    "        value = rewards[1] + next_values[0] \n",
    "        # Return current value and the next values as a single list\n",
    "        return [value] + next_values\n",
    "\n",
    "# TODO (Optional): calculate values from sum\n",
    "def calculate_values_from_sum(rewards):\n",
    "    # Empty list to store the values.\n",
    "    values = []\n",
    "    # TODO: Compute -1st value.\n",
    "    previous_value = np.sum(rewards)\n",
    "    # TODO: Iterate over rewards.\n",
    "    for reward in rewards:\n",
    "        # TODO: Subtract current reward from previous value.\n",
    "        previous_value = previous_value - reward\n",
    "        # TODO: Append the calculated value to values.\n",
    "        values.append(previous_value)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b976ce",
   "metadata": {},
   "source": [
    "Test the `calculate_values` function with some rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2897301d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 9, 7, 4, 0]\n"
     ]
    }
   ],
   "source": [
    "rewards = [0, 1, 2, 3, 4]\n",
    "values = calculate_values(rewards)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea4e59c",
   "metadata": {},
   "source": [
    "(Optional) Test the other value calculation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65be50be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10  9  7  4  0]\n",
      "[10, 9, 7, 4, 0]\n",
      "[10, 9, 7, 4, 0]\n"
     ]
    }
   ],
   "source": [
    "values = calculate_values_cumsum(rewards)\n",
    "print(values)\n",
    "values = calculate_values_recursive(rewards)\n",
    "print(values)\n",
    "values = calculate_values_from_sum(rewards)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fb07f3-8145-4528-a2d3-3700f6c222aa",
   "metadata": {},
   "source": [
    "### Step 8 (TODO): Calculate values from a game \n",
    "Now, play a game and collect the received rewards. Follow the instructions in the code comments. Record the agents positions, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec71f70d-1894-45b1-84fe-2dd54087e1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  1.]\n",
      " [ 0.  8.  0. -1.]\n",
      " [ 4.  0.  0.  0.]]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "move with w, a, s, d; exit with q -  d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went right, received reward 0\n",
      "False\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 0.  8.  0. -1.]\n",
      " [ 0.  4.  0.  0.]]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "move with w, a, s, d; exit with q -  d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went right, received reward 0\n",
      "False\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 0.  8.  0. -1.]\n",
      " [ 0.  0.  4.  0.]]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "move with w, a, s, d; exit with q -  w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went up, received reward 0\n",
      "False\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 0.  8.  4. -1.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "move with w, a, s, d; exit with q -  w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went up, received reward 0\n",
      "False\n",
      "[[ 0.  0.  4.  1.]\n",
      " [ 0.  8.  0. -1.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "move with w, a, s, d; exit with q -  d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went right, received reward 1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# TODO: Initialize environment.\n",
    "env = GridWorld(world_shape, agent_init_pos, blocking_states, reward_states)\n",
    "\n",
    "# TODO: Reset environment and receive initial observaion.\n",
    "obs = env.reset()\n",
    "\n",
    "# List to collect rewards from game.\n",
    "rewards = []\n",
    "# List to collect agent position for later\n",
    "agent_positions = []\n",
    "# Add initial agent position to agent_positions\n",
    "agent_positions.append(env.agent_current_pos)\n",
    "\n",
    "# We loop until the agent completes\n",
    "done = False\n",
    "while not done:\n",
    "    # TODO: Get action from agent.\n",
    "    action = act(obs)\n",
    "    \n",
    "    # TODO: Exit loop if agent's action is exit.\n",
    "    if action == 'exit':\n",
    "        break\n",
    "    \n",
    "    # TODO: Execute action in environment.\n",
    "    obs, reward, done = env.step(action)\n",
    "    # TODO: Store received reward.\n",
    "    rewards.append(reward)\n",
    "    # TODO: Store agents new position.\n",
    "    agent_positions.append(env.agent_current_pos)\n",
    "    \n",
    "    # Print action and reward.\n",
    "    print(f'went {action}, received reward {reward}')\n",
    "    print(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eeb707",
   "metadata": {},
   "source": [
    "Now, compute the values from the recorded rewards. Print rewards, values and agent positions and their lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c50695cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards 5 [0, 0, 0, 0, 1]\n",
      "values 5 [1, 1, 1, 1, 0]\n",
      "agent_positions 6 [(2, 0), (2, 1), (2, 2), (1, 2), (0, 2), (0, 3)]\n"
     ]
    }
   ],
   "source": [
    "values = calculate_values(rewards)\n",
    "print('rewards', len(rewards), rewards)\n",
    "print('values', len(values), values)\n",
    "print('agent_positions', len(agent_positions), agent_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df1aa1c",
   "metadata": {},
   "source": [
    "Notice, that agent_position has one more element than values and rewards. **TODO** Discuss the reason below:\n",
    "#### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43ff78b-380b-4752-828c-eca9b6df4462",
   "metadata": {},
   "source": [
    "### Step 9: Visualize obtained values\n",
    "The function `visualize_values` visualizes the values obtained from a game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d239404-5df4-4c6e-9fec-fab0db378076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_values(values, positions, world_shape):\n",
    "    # Create array containing zeros\n",
    "    value_vis = np.zeros(world_shape)\n",
    "    # Set each position to its corresponding value\n",
    "    for p, v in zip(positions, values):\n",
    "        value_vis[tuple(p)] = v\n",
    "    return value_vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b7081b",
   "metadata": {},
   "source": [
    "Run visualize values on the results of the previous game. Consider, that agent positions has one element more, than values. Feed either `agent_positions[1:]` or `agent_positions[:-1]` in `visualize_values`. Which one is correct? What would happen, if we visited the same position multiple times during our game? **TODO** Discuss below:\n",
    "\n",
    "#### Discussion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59c0ced5-5990-4b94-9bf6-169436b07b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 1., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Visualize the values obtained from the game\n",
    "visualize_values(values, agent_positions[1:], env.world_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff423b21-5714-43cc-92c9-bd2d3f9f3e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
