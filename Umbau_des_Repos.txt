Data/assets:
X- Alle objekte in den ordner "objects
X- "objects_warehouse" und "objects_NOT_WORKING" löschen
X - Aufrufen der gewünschten Objekte über liste in config file
X - Alle Objekte und areas größer machen (einheitliche größe und ein bisschen kleiner als die aktuell großen), min dist passend ändern
X    --> overlapping pose fixen, try-catch und dann halt anzahl objekte reduzieren, falls es nicht geht --> rauslöschen aus object_types liste!!!
X - Alle Objekte und Areas durchgehen und urdf checken, vor allem auf min dist

src/Lib:
X - Ordner "transporter_network" umbennen zu "reinforcement_learning" --> in den config files ändern
X - Dateien "data_generator.py" und "model.py" aus "transporter_network" löschen
X - Klassen aus train_DQN.py: ReplayBuffer, Agent und Supervisor in ein lib python file: lib/reinforcement_learning/DQN_util.py
X    --> in dqnagent abfragen ob supervisor oder nicht (abhängig von netz), supervisor funktioniert nur für abolsute movements, sonst ausschalten!!!
X    --> dqnagent unterschiedliche train functions in abhängigkeit des netzes
X - Klassen aus train_DQN.py: ConvDQN aus den drei Iteration: ResNet, FCN V2, CNN V2 in ein lib python file: lib/reinforcement_learning/ConvDQN.py und dort halt Klassen richtig benennen
X    --> für ResNet checken, ob input_shape durch 8 teilbar
X    --> laden der Gewichte mit Zusatzfunktionen ausstatten, damit nur die passenden Gewichte für das jeweilige Netz geladen werden können sonst User Error message
X    --> choose_action und choose_train function im dqn agenten und damit in train_dqn nur eine funktion die aufgerufen wird und logik passiert direkt in der klasse
X    --> init von dqnAgent clean machen indem load_weights als funktion und initialize convdqn als funktion
X    --> action_dim und input_shape je netz in config definieren
X    --> netztyp in weights file und plots machen
X- Ordner "util" in lib erzeugen
X    --> alle _util.py dort hinverschieben --> passende config files erzeugen

X ==> durchgehen ob einzelne Funktionen irgendwo rumfliegen

Src/scripts_rl:
X - plot functions aus "train_DQN.py" in eigene util file --> "plot_util.py" und in lib verschieben
X - Ordner modells best passende Gewichte für alle drei Netze bereitstellen --> über Dateibenennung zuordnung machen?!

- play_DQN.py betriebsbereit machen und passende config file erzeugen

X - config Ordner: Unterordner pro Ordner/Thema aus lib machen und dort config files verwalten
X - DQN.yaml aufsplitten in sinnvolle Unter-YAMLs. --> bspw. workspace_bounds in bullet_env.yaml schieben und damit auch bei play_game.yaml rausschmeißen
X - Config-Aufbau t.b.d

X - Q-value learning: Alles mit Minima zurück Maxima ändern

X - plots mit flag in config file abfragen was man haben will



X- Logger Status konsitent machen über alle Funktione etc., sodass man über Flag in YAML Modus ändern kann:
X    INFO: Informationen z.B. Episoden-Nummer im Training --> wie YOLO Trainieren



X- UI für User während Training --> vgl. YOLO

X==> imports durchgehen und alles löschen was man nicht braucht
    

Github:
- Alle Branches löschen (lokal und auf github), die nicht mehr gebraucht werden --> die die gebraucht werden sinnvoll umbenennen
- README.me schreiben und Funktion erklären
- Umbau des Repos.txt löschen
- ggf. Aufgabenstellung löschen und in kurzform ins readme
- präsentation und gewichte cnn v2 per email schicken an gergely, hein etc. --> github link schicken (öffentlich machen)