--------------------------------------------------------------------------------------------------------
#                                           ToDo List
--------------------------------------------------------------------------------------------------------
## Priority 1 (sehr hoch)

- Reward: IOU nutzen anstelle von "Orientation-based term as a placeholder for IoU:"

- Reward: Wenn Distanz zwischen Obj & Area vor und nach der Action sich ändert, gibt es einen reward 



## Priority 2
- Netz-Gewichte zwischenspeichern & vorbereiten, dass bei späteren trainings darauf basiert aufgebaut werden kann?

- Beim Modelle zwischenspeichern aufpassen, dass man nichts gute alte Netze nicht überschrieben werden, vllt beim normalen Speichern jeweils mit Uhrzeit/Tag-Namen-Ordner arbeiten, und dann einen "Best"-Ordner pflegen



## Priority 3
- Graue und Schwarze Objekte abfangen, damit kein Problem mit "TCP grau" oder "Unterlagen-Schwarz"

- Multi-Simulation? (mehrere environments gleichzeitig laufen lassen und Buffer füttern?)

- Künstlichen positiven Reward für Bewegungen in falsche Richtung entfernen

- Original ReplayBuffer Klasse und train Funktion in DQNAgent entfernen


## Priority 4 (optional)



--------------------------------------------------------------------------------------------------------
#                                            Erledigt
--------------------------------------------------------------------------------------------------------
- logger prints abhängig von "debug" machen
- Unser environment (alles wo grasp draufsteht, löschen) säubern
- Ein einziges Dockerfile erstellen, das auf TF base besteht, und trotzdem alles von pybullet kann/hat
- Masse, Reibung und Trägheitstensoren anpassen
- Checken, ob TCP perfekt senkrecht & nicht driftet - fixed


